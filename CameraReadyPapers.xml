<?xml version="1.0" encoding="utf-8"?>
<Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet" xmlns:x="urn:schemas-microsoft-com:office:excel" xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet" xmlns:html="http://www.w3.org/TR/REC-html40">
  <Styles>
    <Style ss:ID="s1">
      <Font x:Family="Swiss" ss:Bold="1" />
    </Style>
    <Style ss:ID="s2">
      <NumberFormat ss:Format="0%" />
    </Style>
  </Styles>
  <Worksheet ss:Name="TAIH2021">
    <Table>
      <Row>
        <Cell ss:Index="1">
          <Data ss:Type="String">TAIH2021</Data>
        </Cell>
      </Row>
      <Row ss:Index="3">
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Paper ID</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Paper Title</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Abstract</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Author Names</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Author Emails</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Primary Contact Author Email</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Track Name</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Files</Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Uncertainty-aware INVASE: Enhanced Breast Cancer Diagnosis Feature Selection]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[In this paper, we present an uncertainty-aware INVASE to quantify the predictive confidence of healthcare problems. By introducing learnable Gaussian distributions, we leverage their variances to measure the degree of uncertainty. Based on the vanilla INVASE, two additional modules are proposed, i.e., an uncertainty quantification module in the predictor, and a reward shaping module in the selector. We conduct extensive experiments on UCI-WDBC dataset. Notably, our method eliminates almost all predictive bias with only about 20% queries, while the uncertainty-agnostic counterpart requires nearly 100% queries. The open-source implementation with a detailed tutorial is available at https://github.com/jx-zhong-for-academic-purpose/Uncertainty-aware-INVASE/blob/main/tutorial_invase%2B.ipynb.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jia-Xing Zhong (Department of Computer Science, University of Oxford); hongbo zhang (Department of Computer and Information Sciences, Virginia Military Institute)*]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jxzhong@pku.edu.cn; hbzhang@vt.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hbzhang@vt.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_Workshop_2021.pdf (476531 bytes); AAAI_Workshop_2021__Appendix_.pdf (180461 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Super Learner Model to Detect Abnormalities - OCT and blood smear imaging case studies]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We propose a Super-Learner framework by combining Deep Ensemble Network and feature maps to automate the differential diagnosis from medical imaging datasets. The Super Learner achieved state-of-the-art performance on both imaging datasets and mitigated the chances of over-fitting.
]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Dipam Paul (KIIT University)*; Alankrita Tewari (KIIT University); Imon Banerjee (Emory University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[dipampaul17@gmail.com; alankritat15@gmail.com; imon.banerjee@emory.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[dipampaul17@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_2021.pdf (502784 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Unified Evaluation Of Neural Network Calibration & Refinement]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Network calibration aims at creating deep networks which have predictive confidence representative of their predictive accuracy. Refinement accounts for the degree of separation between a network's correct and incorrect predictions. Both of these properties are highly desired from a deep learning model being deployed in critical settings such as medical analysis, automated driving, etc. However, recent approaches proposed for one have been studied in isolation from the other. In this paper, we aim to evaluate these independently studied solutions together. Firstly, we derive a simple linear relation between the two problems, thereby, linking calibration and refinement. This implies, improving calibration can help achieve a refined model and on the flip side, approaches focused on finding better ordinal ranking of predictions can help in improving calibration of networks. Motivated by this finding, we jointly benchmark various recently proposed approaches for the tasks of calibration and refinement. We find that the existing refinement approaches also provide significant improvement on calibration of the model while maintaining high degree of refinement.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Aditya Singh (Zebra Technologies)*; Alessandro Bay (Zebra Technologies); Andrea Mirabile (Zebra Technologies)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[aditya.singh@zebra.com; alessandro.bay@zebra.com; Andrea.Mirabile@zebra.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[aditya.singh@zebra.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[calib_ref.pdf (244869 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[8]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Analyzing Epistemic and Aleatoric Uncertainty for Drusen Segmentation in Optical Coherence Tomography Images]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Age-related macular degeneration (AMD) is one of the leading causes of permanent vision loss in people aged over 60 years. Accurate segmentation of biomarkers such as drusen that points to the early stages of AMD is crucial in preventing further vision impairment. However, segmenting drusen is extremely challenging due to their varied sizes and appearances, low contrast and noise resemblance. Most existing literature, therefore, have focused on size estimation of drusen using classification, leaving the challenge of accurate segmentation less tackled. Additionally, obtaining the pixel-wise annotations is extremely costly and such labels can often be noisy, suffering from inter-observer and intra-observer variability. Quantification of uncertainty associated with segmentation tasks offers principled measures to inspect the segmentation output. Realizing its utility in identifying erroneous segmentation and the potential applications in clinical decision making, here we develop a U-Net based drusen segmentation model and quantify the segmentation uncertainty. We investigate epistemic uncertainty capturing the model confidence and aleatoric uncertainty capturing the data uncertainty. We present segmentation results and show how uncertainty can help formulate robust evaluation strategies. We visually inspect the pixel-wise uncertainty and segmentation results on test images. We finally analyze the correlation between segmentation uncertainty and accuracy. Our results demonstrate the utility of leveraging uncertainties in developing and explaining
segmentation models for medical image analysis. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tinu Theckel Joy (IBM Research - Australia)*; Suman Sedai (IBM Research Australia); Rahil Garnavi (IBM Research Australia)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[tinu.theckeljoy@ibm.com; ssedai@au1.ibm.com; rahilgar@au1.ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[tinu.theckeljoy@ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Theckel_Joy_aaai2021_taih.pdf (2078663 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[10]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Interpretable Classifiers for Mulit-label Arrhythmia with 12-Lead Electrocardiograms]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Arrhythmia is a serious cardiovascular disease, and in recent years, several artificial intelligence programs have been proposed to automate the arrhythmia diagnosis process. However, most have not been verified on multiple datasets, and they focus on single label diagnosis. What's more concerning, these models conduct arrhythmia diagnosis in a black-box way, which prevents the cardiologists from trusting the computed results. In this study, we propose a multi-label arrhythmia classification algorithm that aims at addressing the aforementioned issues. The developed methodology is composed of three processes: selecting representation, generating features, and predicting outcomes. We developed a cache-inspired method to select a 12-lead electrocardiograms (ECG) heartbeat representation. Moreover, we devised a physiologically interpretable feature generator for segmented 12-lead ECG signals. For multi-label arrhythmia classification, we innovated an efficient arrhythmia outcome prediction procedure that is adaptable to ECG data of variant lengths. Our interpretable multi-label arrhythmia classifier was tested on six publicly available ECG datasets with over 43,000 patients' data, and our model shows the competitiveness with the ranking in the top 7% of the PhysioNet Challenge 2020.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Po-Ya Hsu (UC San Diego)*; Po-Han Hsu (UC San Diego); Tsung-Han Lee (UC San Diego); Hsin-Li Liu (Central Taiwan University of Science and Technology)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p8hsu@eng.ucsd.edu; p6hsu@ucsd.edu; tsl021@eng.ucsd.edu; 101106@gtrial.ctust.edu.tw]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p8hsu@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_Trustworthy_AI_for_healthcare_final-1.pdf (230576 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[11]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Automated Evaluation of Representation in Dermatology Educational Materials]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disparities in dermatological outcomes may be related to inequities in dermatological education, particularly the lack of darker skin images in educational materials used to train dermatologists and primary care physicians. To address this issue, we propose a framework to automatically assess bias in skin tone representation in academic documents of dermatology. Given a document, we apply content parsing to extract text, images, and table cells in a structured format. We then select skin images and segment non-disease regions using Mask R-CNN. 
Individual Typology Angle (ITA) values are computed from non-disease regions and mapped to Fitzpatrick skin indices.  The proposed framework is validated with three dermatology textbooks and compared against manually annotated baselines by dermatology experts.  Results show encouraging performance in estimating skin tones and discover limited representation of darker skins, i.e., only 10.7%,
across these documents. We envision this technology as a tool for dermatology educators to quickly assess their materials.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Girmaw Abebe Tadesse (IBM)*; Hannah Kim (Duke University); Roxana Daneshjou (Stanford University); Celia Cintas (IBM Research); Kush Varshney (IBM Research); Ademide Adelekun (University of Pennsylvania); Jules Lipoff (University of Pennsylvania); Ginikanwa Onyekab (University of Pennsylvania); Veronica Rotemberg (Memorial Sloan-Kettering Cancer Cenrter); James Zou (Stanford University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[girmaw.abebe.tadesse@ibm.com; hannah@cs.duke.edu; roxanad@stanford.edu; Celia.Cintas@ibm.com; krvarshn@us.ibm.com; dollytaiwo@icloud.com; jules.lipoff@pennmedicine.upenn.edu; Ginikanwa.Onyekaba@Pennmedicine.upenn.edu; rotembev@mskcc.org; jamesz@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[girmaw.abebe.tadesse@ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_2021_Trustworthy_camera_ready.pdf (844778 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[13]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[An Attention Mechanism using Multiple Knowledge Sources for COVID-19 Detection from CT Images]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Until now, Coronavirus SARS-CoV-2 has caused more than 850,000 deaths and infected more than 27 million individuals in over 120 countries. Besides principal polymerase chain reaction (PCR) tests, automatically identifying positive samples based on computed tomography (CT) scans can present a promising option in the early diagnosis of COVID-19. Recently, there have been increasing efforts to utilize deep networks for COVID-19 diagnosis based on CT scans. While these approaches mostly focus on introducing novel architectures, transfer learning techniques or construction of large scale data, we propose a novel strategy to improve several performance baselines by leveraging multiple useful information sources relevant to doctors' judgments.  Specifically, infected regions and heat-map features extracted from learned networks are integrated with the global image via an attention mechanism during the learning process. This procedure makes our system more robust to noise and guides the network focusing on local lesion areas.  Extensive experiments illustrate the superior performance of our approach compared to recent baselines. Furthermore, our learned network guidance presents an explainable feature to doctors to understand the connection between input and output in a grey-box model.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Duy  Nguyen Ho Minh (German Research Center for Artificial Intelligence)*; Manh-Duy Nguyen (Dublin City University); Huong Vu Thi (University of California, Berkeley); Thanh Binh Nguyen (University of Science); Fabrizio Nunnari (DFKI GmbH); Daniel Sonntag (DFKI)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[nhmduy.hcmus@gmail.com; manh.nguyen5@mail.dcu.ie; Huong_vu@berkeley.edu; ngtbinh@hcmus.edu.vn; fabrizio.nunnari@dfki.de; daniel.sonntag@dfki.de]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[nhmduy.hcmus@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_Workshop_TrustworthyHealthcare_v3.pdf (1946913 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[17]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Clinical Explainability Failure (CEF) & Explainability Failure Ratio (EFR) – Changing the Way we Validate Classification Algorithms]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Adoption of Artificial Intelligence (AI) algorithms into the clinical realm will depend on their inherent trustworthiness, which is built not only by robust validation studies but is also deeply linked to the explainability and interpretability of the algorithms. Most validation studies for medical imaging AI report the performance of algorithms on study level labels and lay little emphasis on measuring the accuracy of explanations generated by these algorithms in the form of heat maps or bounding boxes, especially in true positive cases. We propose a new metric, Explainability Failure Ratio (EFR), derived from Clinical Explainability Failure (CEF) to address this gap in AI evaluation. We define an Explainability Failure as a case where the classification generated by an AI algorithm matches with study level ground truth but the explanation output generated by the algorithm is inadequate to explain the output of the algorithm. We measured EFR for two algorithms that automatically detect consolidation on chest X rays to determine the applicability of the metric and observed a lower EFR for the model that had lower sensitivity for identifying consolidation on chest X rays, implying that the trustworthiness of a model should be determined not only by routine statistical metrics but also by novel clinically oriented models.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Vasantha Kumar Venugopal (CARING, Mahajan Imaging)*; Vidur Mahajan (CARING, Mahajan Imaging Centre); Rohit Takhar (CARING, Mahajan Imaging); Salil Gupta (CARING, Mahajan Imaging)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[vasanth.venugopal@caring-research.com; vidur@mahajanimaging.com; rohit.takhar@caring-research.com; Salil.gupta@caring-research.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[vasanth.venugopal@caring-research.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[18]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Predicting Hyperkalemia in the ICU and Evaluation of Generalizability and Interpretability]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hyperkalemia is a potentially life-threatening condition that can lead to fatal arrhythmias. Early identification of high risk patients can inform clinical care to mitigate the risk. While hyperkalemia is often a complication of acute kidney injury (AKI), it also occurs in the absence of AKI. We developed predictive models to identify intensive care unit (ICU) patients at risk of developing hyperkalemia by using the Medical Information Mart for Intensive Care (MIMIC) and the eICU Collaborative Research Database (eICU-CRD). Our methodology focused on building multiple models, optimizing for interpretability through model selection, and simulating various clinical scenarios.
In order to determine if our models perform accurately on patients with and without AKI, we evaluated the following clinical cases: (i) predicting hyperkalemia after AKI within 14 days of ICU admission, (ii) predicting hyperkalemia within 14 days of ICU admission regardless of AKI status, and compared different lead times for (i) and (ii). Both clinical scenarios were modeled using logistic regression (LR), random forest (RF), and XGBoost. 
Using observations from the first day in the ICU, our models were able to predict hyperkalemia with an AUC of (i) 0.79, 0.81, 0.81 and (ii) 0.81, 0.85, 0.85 for LR, RF, and XGBoost respectively. We found that 4 out of the top 5 features were consistent across the models. AKI stage was significant in the models that included all patients with or without AKI, but not in the models which only included patients with AKI. This suggests that while AKI is important for hyperkalemia, the specific stage of AKI may not be as important. Our findings require further investigation and confirmation. 
]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hyunjung Kwak (Hong Kong University of Science and Technology)*; Christina Chen (MIT); Lowell Ling (CUHK); Erina Ghosh (Philips Research North Americs); Leo Celi (BIDMC); Pan Hui (Hong Kong University of Science and Technology)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hkwak@cse.ust.hk; christinium@gmail.com; lowell.ling@cuhk.edu.hk; erina.ghosh@philips.com; leoanthonyceli@yahoo.com; panhui@cse.ust.hk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hkwak@cse.ust.hk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_Press_Hyperkalemia_camera_ready.pdf (1741017 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[19]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Explainability Matters: Backdoor Attacks on Medical Imaging]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Deep neural networks have been shown to be vulnerable to backdoor attacks, which could be easily introduced to the training set prior to model training. Recent work has focused on investigating backdoor attacks on natural images or toy datasets. Consequently, the exact impact of backdoors is not yet fully understood in complex real-world applications, such as in medical imaging where misdiagnosis can be very costly. In this paper, we explore the impact of backdoor attacks on a multilabel disease classification task using chest radiography, with the assumption that the attacker can manipulate the training dataset to execute the attack. Extensive evaluation of a state-of-the-art architecture demonstrates that by introducing images with few-pixel perturbations into the training set, an attacker can execute the backdoor successfully without having to be involved with the training procedure. A simple 3×3 pixel trigger can achieve up to 1.00 Area Under the Receiver Operating Characteristic (AUROC) curve on the set of infected images. In the set of clean images, the backdoored neural network could still achieve up to 0.85 AUROC, highlighting the stealthiness of the attack. As the use of deep learning based diagnostic systems proliferates in clinical practice, we also show how explainability is indispensable in this context, as it can identify spatially localized backdoors in inference time.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Munachiso Nwadike (New York University)*; Takumi Miyawaki (New York University Abu Dhabi); Esha Sarkar (NYU Tandon School of Engineering); Michail Maniatakos (New York University Abu Dhabi); Farah Shamout (New York University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[msn307@nyu.edu; tm2904@nyu.edu; esha.sarkar@nyu.edu; michail.maniatakos@nyu.edu; fs999@nyu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[msn307@nyu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[aaai_camera_ready-compressed.pdf (275620 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[20]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Explanation Strategies for Trustworthy AI Diagnostic Systems: Examining Physicians’ Explanatory Reasoning in Re-diagnosis Scenarios ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AI systems are increasingly being deployed to provide the first point of contact for patients. These systems are typically focused on question-answering, and suffer from many of the same deficiencies in explanation that have plagued medical diagnostic systems since the 1970s (Shortliffe, Buchanan, and Feigenbaum 1979). They provide information that patients or physicians may not need or would prefer to get in other ways. To provide better guidance about explanations in these systems, we report on an interview study in which we identified explanations that physicians used in the context of a re-diagnosis or a change in diagnosis. Five broad categories of explanation emerged: 1) explanations intended to prepare the patient for later possibilities; 2) ways to tailor information to the audience; 3) use of case information to make a logical argument, 4) use of test results and logical constructs to support the diagnosis; and 5) communication intended to build emotional connection and rapport. We also present these in a diagnosis meta-timeline that identifies points at which we observed explanatory reasoning strategies. Altogether, this study suggests explanation strategies, approaches, and methods that might be used by medical diagnostic AI systems to improve user trust and satisfaction with these systems.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Lamia Alam (Michigan Technological University )*]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[lalam@mtu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[lalam@mtu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_TAIH2021.pdf (599742 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[22]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Designing for AI Explainability in Clinical Context]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The growing use of artificial intelligence in medical settings has led to increased interest in AI Explainability (XAI).  While research on XAI has largely focused on the goal of increasing users’ appropriate trust and application of insights from AI systems, we see intrinsic value in explanations themselves (and the role they play in furthering clinician’s understanding of a patient, disease, or system).  Our research studies explanations as a core component of bi-directional communication between the user and AI technology. As such, explanations must be understood and evaluated in context, reflecting the specific questions and information needs that arise in actual use. In this paper, we present a framework and approach for anticipating XAI needs during the development of a human-centered AI system. We illustrate this approach through a user study and design prototype, which situated endocrinologists in a clinical setting involving guideline-based diabetes treatment. Our results show the variety of explanation types needed in clinical settings, the usefulness of our approach for identifying these needs early while a system is still being designed, and the importance of keeping humans in the loop during both the development and use of AI systems.
]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Daniel Gruen (RPI)*; Shruthi Chari (RPI); Oshani Seneviratne (RPI); Deborah McGuinness (Rensselaer Polytechnic Institute); Morgan Foreman (IBM); Amar Das (IBM)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[gruend2@rpi.edu; charis@rpi.edu; senevo@rpi.edu; dlm@cs.rpi.edu; morgan.foreman@ibm.com; amardasmdphd@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[gruend2@rpi.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Gruen et al AAAI Workshop Camera Ready.pdf (635072 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[23]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Electronic Health Records (EHRs) provide a wealth of information for machine learning algorithms to predict the patient outcome from the data including diagnostic information, vital signals, lab tests, drug administration, and demographic information. Machine learning models can be built, for example, to evaluate patients based on their predicted mortality or morbidity and to predict required resources for efficient resource management in hospitals. In this paper, we demonstrate that an attacker can manipulate the machine learning predictions with EHRs easily and selectively at test time by backdoor attacks with the poisoned training data. Furthermore, the poison we create has statistically similar features to the original data making it hard to detect, and can also attack multiple machine learning models without any knowledge of the models. With less than 5% of the raw EHR data poisoned, we achieve average attack success rates of 97% on mortality prediction tasks with MIMIC-III database against Logistic Regression, Multilayer Perceptron, and Long Short-term
Memory models simultaneously]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Byunggill Joe (KAIST)*; Akshay Mehra (Tulane University); Insik Shin (KAIST); Jihun Hamm (Tulane University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[cp4419@kaist.ac.kr; amehra@tulane.edu; insik.shin@kaist.ac.kr; jhamm3@tulane.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[cp4419@kaist.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Adversarial_medical_data__AAAI_2020_Workshop_appendix.pdf (735364 bytes); Adversarial_medical_data__AAAI_2020_Workshop_main.pdf (486593 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[25]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Towards Cotenable and Causal Shapley Feature Explanations]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[A key component of being able to trust machine learning models in medical contexts is the ability to explain why the model is making a particular prediction. Feature importance methods based on Shapley values have become popular, but there has also been recent debate surrounding whether their mathematical properties may limit their use for expaining models. We outline some properties that a model explanation needs in order to be useful: model explanations should be \textit{causal}, as ultimately they are used to aid in decision making, and they should be \textit{cotenable}; they should respect the observed correlation between features. We show how different implementations of Shapley-based feature importances trade off these properties and propose using medical domain knowledge to group features as a step towards satisfying both causality and cotenability, which would provide model explanations that are more useful in clinical settings.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tony Liu (University of Pennsylvania)*; Lyle Ungar (University of Pennsylvania)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[liutony@seas.upenn.edu; ungar@cis.upenn.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[liutony@seas.upenn.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Medical_Shapley_TAIH_2020_camera_ready.pdf (395875 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[26]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Towards Fairness in Classifying Medical Conversations into SOAP Sections]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[As machine learning algorithms are more widely deployed in healthcare, the question of algorithmic fairness becomes more critical to examine. Our work seeks to identify and understand disparities in a deployed model that classifies doctor-patient conversations into sections of a medical SOAP note. We employ several metrics to measure disparities in the classifier performance, and find small differences in a portion of the disadvantaged groups. A deeper analysis of the language in these conversations and further stratifying the groups suggests these differences are related to and often attributable to the type of medical appointment (e.g., psychiatric vs. internist). Our findings stress the importance of understanding the disparities that may exist in the data itself and how that affects a model's ability to equally distribute benefits.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Elisa Ferracane (Abridge AI Inc)*; Sandeep Konam (Abridge AI Inc)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[elisa@abridge.com; san@abridge.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[elisa@abridge.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Bias_AAAI_Trustworthy_Workshop.pdf (189163 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[27]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Graphical Models For Rare Sequence Variant Interpretation]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Interpretation of rare sequence variants is a key challenge in clinical genetic testing. In the absence of a definitive model to ascertain variant pathogenicity interpretation is usually conducted by combining evidence from multiple sources via heuristic rules and points-based systems. In this paper, we explore a fundamentally different modeling approach – one based on probabilistic graphical models. We present initial attempts at graphical modeling of the variant interpretation task, highlighting the benefits such as transparency of modeling assumptions, explainability, sensitivity analysis, etc. while also describing challenges that are to be overcome]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Arun Nampally (Invitae)*; Eugene Palovcak (Invitae); Garrett Bernstein (Invitae); Matthew Davis (Invitae)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[arun.nampally@invitae.com; eugene.palovcak@invitae.com; garrett.bernstein@invitae.com; matthew.davis@invitae.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[arun.nampally@invitae.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[main.pdf (123309 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[28]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Combining Aleatoric and Epistemic Uncertainties for Robust Healthcare Decision-Making]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Automating and improving healthcare decision-making using machine learning is limited by the inability for practitioners to trust decisions handed to them by black-box models. Other work has explored making model decisions more interpretable, but this may reduce model accuracy, and often requires expertise to determine cases where the model is unreliable (such as when data are encountered that aren't represented in the training set). This paper presents an approach to unify two types of uncertainty in the context of regression problems, giving the novel model the ability to provide accurate per-instance confidence regions, without compromising on model accuracy. These techniques are evaluated in terms of likelihood of the true data under the confidence regions and ability to distinguish out-of-distribution test points. This paper also shows that the techniques presented robustly distinguish two types of uncertainty: uncertainty due to inherent variability (aleatoric risk) and uncertainty due to a lack of experience (epistemic uncertainty).]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zhiyin Lin (Oregon Episcopal School); Samuel Saarinen (Brown University)*; Michael Littman (Brown University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[linz@go.oes.edu; sam_saarinen@brown.edu; mlittman@cs.brown.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sam_saarinen@brown.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_21_W26_CameraReady.pdf (239771 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[29]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Bias in Clinical Risk Prediction Models: Challenges in Application to Observational Health Data]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[With the increasing use of machine learning and AI in healthcare, ensuring the fairness of algorithms is paramount to prevent health disparities and inequities from being reproduced in the algorithm-guided medical and policy decisions. In this work we investigate algorithmic bias in clinical prediction models and discuss challenges in analyzing bias in observational health data. We show that potential disparities in treatment opportunity exist between races in the data for patients with opioid use disorder, and that the direction of bias favoring one race over the other depends on the choice of outcome label or fairness metric. We further demonstrate how debiasing algorithms can effectively mitigate the apparent bias in most experimental settings. This study exemplifies the importance of thorough bias assessment in prediction tasks based on healthcare data.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yoonyoung Park (IBM Research)*; Moninder Singh (IBM Research); Issa Sylla (IBM Research); Elaine Xiao (IBM Research); Jianying Hu (IBM); Amar Das (IBM Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yoonyoung.park@ibm.com; moninder@us.ibm.com; issa.sylla@ibm.com; eyxiao@mit.edu; jyhu@us.ibm.com; amar.das@merck.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yoonyoung.park@ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[camera_AAAI2020_Fairness_OUD.pdf (348026 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[30]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Towards Verifying Results from Biomedical NLP Machine Learning Models Using the UMLS: Cases of Classification and Named Entity Recognition]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Machine learning (ML) for biomedical research is one of the fastest growing research areas in the world today. For NLP specifically, free-text healthcare reports are an important resource whose processing can contribute potentially to patient diagnosis, treatment, and management. However, the inability to explain the outputs of ML algorithms is currently a barrier to the use of these models in a clinical setting. We present a method that uses the ontologies and knowledgebases in the Unified Medical Language System (UMLS) to verify and/explain the output of biomedical ML models. Our verifier takes as input the results from an ML model, and uses the UMLS to correlate the results of the task with the confidence of the model for each result. We applied this architecture to two tasks using textual cancer pathology reports: ICD-O topography classification, and named entity recognition. For the former, we identified that the presence of certain entities in a report is inversely related to the model’s confidence values; while, for the latter, we identified categories of errors related to lower confidence values. Our approach, therefore, not only verifies the accuracy of ML model results, but provides explanations that may be used to improve model design and performance.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Waheeda Saib (IBM Research)*; Joan Byamugisha (IBM Research); Maletsabisa Molapo (IBM Research); Theodore Gaelejwe (IBM Research); Asad Jeewa (IBM Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[WSaib@za.ibm.com; joan.byamugisha@ibm.com; maletsabisa.molapo@ibm.com; theodore.gaelejwe@ibm.com; asad.jeewa@ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[WSaib@za.ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[31]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TF-IDF Weighted Similarity Estimates for Unseen Categories]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Results  of  machine  learning  models  for  clinical  data are  difficult  to  generalize  outside  of  the  context  from which the data was gathered due to substantial differences in the target population with respect to geography, demography, etc. Naturally applying a prediction mode in a new context becomes problematic since the train and test data differ in distribution and a common pit-fall in such a procedure is handling unseen categories.This  is  exacerbated  in  electronic  health  records  data due to the large set of possible categorical occurrences such  as  ICD-10  and  CPT  codes.  Modern  approaches rely on preprocessing techniques such as imputation by treating unseen categories as missing values or by as-signing them predetermined clusters. TF-IDF SimilarityWeighted  Estimates  (TIWS)  is  a  novel  framework  by treating categorical data in an NLP context. TIWS as-signs the unseen category a linear combination of seen categories with weights based on similarity measures.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Handong Bang (Ancestry)*; Feng-Chang Lin (UNC Chapel Hill); Michael Kosorok (UNC Chapel Hill); Alex Comerfield (Bloomberg)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[handavidbang@gmail.com; flin33@email.unc.edu; kosorok@unc.edu; acomerford3@bloomberg.net]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[handavidbang@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_TIWS_WKSHP_camera.pdf (171605 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[32]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Reducing Risk and Uncertainty of Deep Neural Networks on Diagnosing COVID-19 Infection]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Effective and reliable screening of patients via Computer-Aided Diagnosis can play a crucial part in the battle against COVID-19. Most of the existing works focus on developing sophisticated methods yielding high detection performance, yet not addressing the issue of predictive uncertainty. In this work, we introduce uncertainty estimation to detect confusing cases for expert referral to address the unreliability of state-of-the-art (SOTA) DNNs on COVID-19 detection. To the best of our knowledge, we are the first to address this issue on the COVID-19 detection problem. In this work, we investigate a number of SOTA uncertainty estimation methods on publicly available COVID dataset and present our experimental findings. In collaboration with medical professionals, we further validate the results to ensure the viability of the best performing method in clinical practice.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Krishanu Sarker (Georgia State University)*; Sharbani Pandit (Georgia Institute of Technology); Anupam  Sarker (Institute of Epidemiology, Disease Control and Research); Saeid Belkasim (Georgia State university); Shihao Ji (Georgia State University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ksarker1@student.gsu.edu; pandit@gatech.edu; anupam.sarker639@gmail.com; sbelkasim@gsu.edu; sji@gsu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ksarker1@student.gsu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI_TAIH21_supplementary.pdf (795300 bytes); AAAI_TAIH21.pdf (3258674 bytes); AAAI_TAIH21-merged (1).pdf (4065255 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[33]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Algorithmic Selection of Patients for Case Management: Alternative Proxies to Healthcare Costs]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Expected healthcare costs are commonly used in the United
States as a proxy for health to select patients for case management.
However, several recent studies have shown that AI
algorithms for predicting costs exacerbate underlying disparities
in the healthcare system and result in substantial bias
against blacks, who have to be much sicker than whites to be
chosen by these algorithms. We look at an alternative proxy
based on emergency-room and inpatient utilization, and show
that it results in more fair outcomes, reducing racial disparity
while choosing patients truly in need for such services. We
evaluate the effectiveness of this approach using the publicly
available and nationally representative Medical Expenditure
Panel Survey data collected annually by the U.S. Department
of Health and Human Services.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Moninder Singh (IBM Research)*]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[moninder@us.ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[moninder@us.ibm.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[AAAI2021_MEPS_Camera_Ready_Singh.pdf (62176 bytes)]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="String"><![CDATA[34]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ethical Issues in Biomarker Discovery for Precision Psychiatry]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TBA]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Manjari Narayan (Stanford University)*]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[manjari@alumni.rice.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[manjari@alumni.rice.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[TAIH2021]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
    </Table>
  </Worksheet>
</Workbook>